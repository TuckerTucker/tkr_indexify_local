

 ==== __init__.py ====

from .logger import *

 ==== extractor_downloader.py ====

import json
import os
import subprocess
import requests
from typing import Optional
import logging

logging.basicConfig(filename='app.log', level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')

def download_all_extractors(url: str = None, output_file: str = "indexify-local/extractors-json/all_extractors.json") -> None:
    """
    Download the JSON file containing all extractors from the specified URL
    or the default GitHub URL and save it locally.
    """
    if url is None:
        url = "https://raw.githubusercontent.com/tensorlakeai/indexify-extractors/main/extractors.json"

    try:
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        response = requests.get(url)
        response.raise_for_status()

        data = response.json()
        with open(output_file, "w") as file:
            json.dump(data, file, indent=2)

        logging.info(f"Successfully downloaded all extractors to {output_file}")
        print(f"Successfully downloaded all extractors to {output_file}")
    except requests.exceptions.RequestException as e:
        logging.error(f"Error downloading all extractors: {e}")
        print(f"Error downloading all extractors: {e}")
    except json.JSONDecodeError as e:
        logging.error(f"Error parsing JSON data: {e}")
        print(f"Error parsing JSON data: {e}")
    except IOError as e:
        logging.error(f"Error writing to file: {e}")
        print(f"Error writing to file: {e}")


def update_local_extractors_list(input_file: str, output_file: str = "indexify-local/extractors-json/local_extractors.json") -> None:
    """
    Update the local extractors list by adding new extractors from the input file if they are not already present.
    Adjusted to handle the format of whisper_chain.json.
    """
    logging.info("Starting update of local extractors list")
    try:
        if not os.path.exists(input_file):
            logging.error(f"Input file {input_file} does not exist.")
            print(f"Error: Input file {input_file} does not exist.")
            return

        with open(input_file, "r") as file:
            data = json.load(file)
            new_extractors = data.get("extractors", [])  # Adjusted to handle nested structure
            if not isinstance(new_extractors, list) or not all(isinstance(extractor, dict) for extractor in new_extractors):
                raise ValueError("Input file does not contain a valid list of extractor dictionaries.")

        if not os.path.exists(output_file):
            existing_extractors = []
        else:
            with open(output_file, "r") as file:
                existing_extractors = json.load(file)
                if not isinstance(existing_extractors, list) or not all(isinstance(extractor, dict) for extractor in existing_extractors):
                    existing_extractors = []

        updated = False
        for new_extractor in new_extractors:
            if new_extractor not in existing_extractors:
                existing_extractors.append(new_extractor)
                updated = True

        if updated:
            with open(output_file, "w") as file:
                json.dump(existing_extractors, file, indent=4)
                logging.info(f"Updated local extractors list and written to {output_file}.")
                print(f"Updated local extractors list and written to {output_file}.")
        else:
            logging.info("No updates needed.")
            print("No updates needed.")

    except json.JSONDecodeError as e:
        logging.error(f"Error parsing JSON data: {e}")
        print(f"Error parsing JSON data: {e}")
    except IOError as e:
        logging.error(f"Error writing to file {output_file}: {e}")
        print(f"Error writing to file {output_file}: {e}")
    except ValueError as e:
        logging.error(f"Validation error: {e}")
        print(f"Validation error: {e}")
    except Exception as e:
        logging.error(f"An unexpected error occurred: {str(e)}")
        print(f"An unexpected error occurred: {str(e)}")

def download_extractors(input_file: str) -> None:
    """
    Download extractors specified in the local extractors JSON file and update the list.
    Adjusted to handle the format of whisper_chain.json.
    """
    logging.info("Starting download of extractors")

    if not os.path.exists(input_file):
        logging.error(f"The specified file '{input_file}' does not exist.")
        print(f"Error: The specified file '{input_file}' does not exist.")
        return

    try:
        with open(input_file, "r") as file:
            data = json.load(file)
            extractors = data.get("extractors", [])

        for extractor in extractors:
            if "module_name" not in extractor or "type" not in extractor:
                logging.error("Missing required fields in extractor definition.")
                print("Error: Missing required fields in extractor definition.")
                continue

            module_name = extractor["module_name"]
            module = module_name.split(".")[0]
            extractor_type = extractor["type"]
            cli_call = f"indexify-extractor download hub://{extractor_type}/{module}"

            try:
                subprocess.run(cli_call, shell=True, check=True)
                update_local_extractors_list(input_file)
                logging.info(f"Successfully downloaded extractor: {module_name}")
            except subprocess.CalledProcessError as e:
                logging.error(f"Error downloading extractor {module_name}: {e}")
                print(f"Error downloading extractor {module_name}: {e}")

    except json.JSONDecodeError as e:
        logging.error(f"Error parsing JSON data: {e}")
        print(f"Error parsing JSON data: {e}")
    except Exception as e:
        logging.error(f"An unexpected error occurred: {str(e)}")
        print(f"An unexpected error occurred: {str(e)}")

    logging.info("Completed download of extractors")

 ==== indexify_config_generator.py ====

import os

def create_indexify_config(template_file, output_file):
    """
    Generate the Indexify configuration file based on the provided template.

    Args:
        template_file (str): Path to the template file.
        output_file (str): Path to save the generated configuration file.
    """
    try:
        # Read the template file
        with open(template_file, 'r') as template_file:
            template_content = template_file.read()

        # Get the full path of the destination directory's parent directory
        destination_dir = os.path.dirname(os.path.abspath(os.path.dirname(output_file)))

        # Replace the '../' placeholders with the full path of the destination directory's parent directory
        updated_content = template_content.replace('../', destination_dir + '/')

        # Create the output directory if it doesn't exist
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        # Write the updated content to the output file
        with open(output_file, 'w') as output_file:
            output_file.write(updated_content)

        print(f"Indexify configuration file created at: {output_file.name}")
    except FileNotFoundError as e:
        print(f"Template file not found: {template_file}")
        print(f"Error: {e}")
    except IOError as e:
        print(f"Error writing to file: {output_file.name}")
        print(f"Error: {e}")

 ==== logger.py ====

import logging

def configure_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('_local_data/xfi-local.log'),
            logging.StreamHandler()
        ]
    )

 ==== mime_types.py ====

# Used by watch_folder.py
class MimeTypes:
    MIMES = [
        #documents
        "application/pdf",
        
        #Audio
        "audio/mpeg",
        "audio/wav",
        "audio/x-wav",
        "audio/flac",
        "audio/aac",
        "audio/aacp",
        "audio/ogg",
        "audio/x-ms-wma",
        "audio/aiff",
        "audio/x-aiff",
        "audio/mp4",
        "audio/x-m4a",
        "audio/amr"
    ]

 ==== new_local.py ====

import json
from indexify import IndexifyClient
from xfi.supervisord_conf_generator import generate_supervisord_conf
from xfi.indexify_config_generator import create_indexify_config

class NewLocal:
    def __init__(self, recipe_path):
        with open(recipe_path, 'r') as file:
            recipe = json.load(file)
            self.extractors = recipe['extractors']
            self.chain_name = recipe['name']

    def generate_policy(self):
        client = IndexifyClient()
        i=0
        for i, extractor in enumerate(self.extractors):
            module = extractor["module_name"].split(".")[0]
            extractor_name = f"tensorlake/{module}"  # This will need to change when custom extractors are being used

            if i == 0:
                content_source = "ingestion"
            else:
                prev_extractor = self.extractors[i-1]["module_name"].split(".")[0]
                content_source = f"{prev_extractor}-{self.chain_name}"

            client.add_extraction_policy(
                extractor=extractor_name,
                name=f"{module}-{self.chain_name}",
                content_source=content_source
            )
        self.extraction_policies = client.extraction_policies

    def update_configs(self, templates, input_file):
        # Update supervisor config
        generate_supervisord_conf(
            input_file=input_file,
            output_file=templates['supervisord']['output_path'],
            template_file=templates['supervisord']['template_path'],
            chain_name=self.chain_name
        )

        # Update indexify config
        create_indexify_config(
            template_file=templates['indexify']['template_path'],
            output_file=templates['indexify']['output_path']
        )

 ==== script_runner.py ====

import subprocess

def run_bash_script(script_path):
    """
    Run the specified bash script using subprocess.

    Args:
        script_path (str): Path to the bash script.

    Raises:
        subprocess.CalledProcessError: If there's an error executing the bash script.
    """
    try:
        # Run the bash script using subprocess
        subprocess.run(["bash", script_path], check=True)
        print(f"Bash script executed successfully: {script_path}")
    except subprocess.CalledProcessError as e:
        print(f"Error executing bash script: {script_path}")
        print(f"Error message: {e}")

def run_default_scripts():
    """
    Run the default bash scripts specified in the `default_scripts` list.
    """
    default_scripts = ["start_xfi"]
    for script in default_scripts:
        run_bash_script(script)

 ==== setup.py ====

import subprocess
from xfi.extractor_downloader import download_all_extractors

def setup():
    """
    Run the indexify-local/get_latest script and download the all_extractors.json file.
    """
    try:
        
        # Run the get_latest script using subprocess with the specified working directory
        subprocess.run(["bash", "get_latest"], cwd="indexify-local", check=True)
        print("get_latest script executed successfully")
    except subprocess.CalledProcessError as e:
        print(f"Error executing get_latest script: {e}")

    # Download the all_extractors.json file
    download_all_extractors()

def reset():
    """
    Clear all logs and config files
    """
    try:
        subprocess.run(["bash", "reset_local"], cwd="utils", check=True)
        print("reset_local script executed successfully")
    except subprocess.CalledProcessError as e:
        print(f"Error executing reset_local script: {e}")

def clear_logs():
    """
    Clear all logs
    """
    try:
        subprocess.run(["bash", "clear_logs"], cwd="utils", check=True)
        print("clear_logs script executed successfully")
    except subprocess.CalledProcessError as e:
        print(f"Error executing reset_local script: {e}")

 ==== supervisord_conf_generator.py ====

import json

def generate_supervisord_conf(input_file, output_file, template_file, chain_name='default_chain'):
    try:
        with open(input_file, "r") as file:
            data = json.load(file)
            if data[0]: 
                extractors = data
            else:
                extractors = data["extractors"]
                

        extractor_programs = ""
        for extractor in extractors:
            module_name = extractor["module_name"]
            module = module_name.split(".")[0]
            extractor_programs += f"""
            [program:{chain_name}-{module}]
            command=indexify-extractor join-server {module_name} --coordinator-addr localhost:8950 --ingestion-addr localhost:8900
            autostart=true
            autorestart=true
            stderr_logfile=_local_data/supervisor/logs/{chain_name}-{module}.err.log
            stdout_logfile=_local_data/supervisor/logs/{chain_name}-{module}.out.log
            priority=3
            """

        with open(template_file, "r") as file:
            template = file.read()

        supervisord_conf = template.replace("{extractor_programs}", extractor_programs.strip())

        with open(output_file, "w") as file:
            file.write(supervisord_conf)

        print(f"Generated supervisord.conf at {output_file}")
    except FileNotFoundError as e:
        print(f"File not found: {e}")
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON data: {e}")
    except IOError as e:
        print(f"Error writing to file: {e}")

 ==== templates.py ====

import json

def load_templates(templates_path="templates/templates.json"): 
    with open(templates_path, 'r') as file:
        return json.load(file)

 ==== watch_folder.py ====

import os
import time
import mimetypes
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from indexify import IndexifyClient
from mime_types import MimeTypes
from logger import configure_logging
import logging, logger

# Configure logging
configure_logging()

class FileHandler(FileSystemEventHandler):
    logging.info("Starting: FileHandler")
    def on_created(self, event):
        try:
            if event.is_directory:
                logging.info(f"Processing: Directory {event.src_path}")
                self.process_directory(event.src_path)
            else:
                logging.info(f"Processing: File {event.src_path}")
                self.process_file(event.src_path)
        except Exception as e:
            logging.error(f"Error processing event: {event}")
            logging.exception(e)  # Log the full exception traceback    

    def process_directory(self, directory_path):
        try:
            logging.info(f"New directory detected: {directory_path}")
            for root, dirs, files in os.walk(directory_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    self.process_file(file_path)
        except Exception as e:
            logging.error(f"Error processing directory: {directory_path}")
            logging.exception(e)

    def process_file(self, file_path):
        try:
            file_directory, file_name = os.path.split(file_path)
            file_size = os.path.getsize(file_path)
            file_size_mb = file_size / (1024 * 1024)
            mime_type, _ = mimetypes.guess_type(file_path)

            if mime_type in MimeTypes.MIMES:
                logging.info(f"Processing file: {file_name}")
                logging.info(f"File size: {file_size_mb:.2f} MB")
                logging.info(f"File type: {mime_type}")

                client = IndexifyClient()
                client.upload_file(path=file_path)
            else:
                logging.warning(f"Skipping {file_name} (MIME type: {mime_type})")

        except Exception as e:
            logging.error(f"Error processing file: {file_path}")
            logging.exception(e)

def watch_folder(folder_path):
    event_handler = FileHandler()
    observer = Observer()
    observer.schedule(event_handler, folder_path, recursive=True)
    observer.start()

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
    except Exception as e:
        print(f"Error in folder watching: {folder_path}")
        print(f"Error details: {str(e)}")
    finally:
        observer.join()

# Specify the folder path to watch
folder_to_watch = "_watch_folder"

# Start watching the folder
try:
    logging.info(f"Watching: Folder {folder_to_watch}")
    watch_folder(folder_to_watch)
except Exception as e:
    print(f"Error starting folder watching: {folder_to_watch}")
    print(f"Error details: {str(e)}")